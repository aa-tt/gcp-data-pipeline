#!/bin/bash
#
# Test script for GCP Data Pipeline
# Tests each component of the pipeline
#

set -e

ENVIRONMENT=${1:-dev}
PROJECT_ID=${GCP_PROJECT_ID}
REGION=${GCP_REGION:-us-central1}

echo "üß™ Testing GCP Data Pipeline"
echo "   Environment: $ENVIRONMENT"
echo ""

# Test 1: Publish test message to Pub/Sub
echo "üì§ Test 1: Publishing test message to Pub/Sub..."
gcloud pubsub topics publish raw-data-topic-${ENVIRONMENT} \
    --message='{"test": "data", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}'
echo "‚úÖ Message published"

# Test 2: Call Cloud Function
echo ""
echo "üì§ Test 2: Testing Cloud Function..."
FUNCTION_URL=$(gcloud functions describe data-ingestion-${ENVIRONMENT} \
    --region=$REGION \
    --gen2 \
    --format="value(serviceConfig.uri)" 2>/dev/null || echo "")

if [ -n "$FUNCTION_URL" ]; then
    curl -X POST "$FUNCTION_URL" \
        -H "Content-Type: application/json" \
        -d '{
            "data_type": "test-transaction",
            "source_system": "test-system",
            "payload": {
                "user_id": "user-123",
                "product_id": "prod-456",
                "amount": 99.99,
                "quantity": 2,
                "category": "electronics",
                "region": "us-west"
            }
        }'
    echo ""
    echo "‚úÖ Cloud Function called"
else
    echo "‚ö†Ô∏è  Cloud Function not deployed yet"
fi

# Test 3: Check Cloud Storage buckets
echo ""
echo "üì¶ Test 3: Checking Cloud Storage buckets..."
for bucket in raw-data processed-data staging; do
    if gsutil ls gs://${PROJECT_ID}-${bucket}-${ENVIRONMENT} > /dev/null 2>&1; then
        echo "‚úÖ Bucket gs://${PROJECT_ID}-${bucket}-${ENVIRONMENT} exists"
    else
        echo "‚ùå Bucket gs://${PROJECT_ID}-${bucket}-${ENVIRONMENT} not found"
    fi
done

# Test 4: Check BigQuery tables
echo ""
echo "üìä Test 4: Checking BigQuery tables..."
for table in raw_data analytics_data daily_metrics; do
    if bq show ${PROJECT_ID}:data_warehouse_${ENVIRONMENT}.${table} > /dev/null 2>&1; then
        echo "‚úÖ Table ${table} exists"
        # Show row count
        COUNT=$(bq query --use_legacy_sql=false --format=csv \
            "SELECT COUNT(*) as count FROM ${PROJECT_ID}.data_warehouse_${ENVIRONMENT}.${table}" 2>/dev/null | tail -1)
        echo "   Records: $COUNT"
    else
        echo "‚ùå Table ${table} not found"
    fi
done

# Test 5: Submit a test PySpark job
echo ""
echo "üî• Test 5: Submitting test PySpark job..."
read -p "Submit test PySpark job? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    gcloud dataproc batches submit pyspark \
        gs://${PROJECT_ID}-staging-${ENVIRONMENT}/pyspark-jobs/etl_transform.py \
        --region=$REGION \
        --service-account=data-pipeline-sa-${ENVIRONMENT}@${PROJECT_ID}.iam.gserviceaccount.com \
        -- \
        --project-id=$PROJECT_ID \
        --environment=$ENVIRONMENT \
        --date=$(date -u +%Y-%m-%d)
    echo "‚úÖ PySpark job submitted"
else
    echo "‚è≠Ô∏è  Skipped PySpark job submission"
fi

echo ""
echo "‚úÖ Testing complete!"
