name: Deploy GCP Data Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
  workflow_dispatch:

env:
  GCP_REGION: us-central1
  TERRAFORM_VERSION: 1.6.0

jobs:
  terraform-validate:
    name: Validate Terraform
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Format Check
        working-directory: ./terraform
        run: terraform fmt -check -recursive

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init -backend=false

      - name: Terraform Validate
        working-directory: ./terraform
        run: terraform validate

  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'config'
          scan-ref: 'terraform/'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: [terraform-validate, security-scan]
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: ./terraform
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_region: ${{ env.GCP_REGION }}
          TF_VAR_environment: dev
        run: terraform plan -out=tfplan

      - name: Upload Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan
          path: terraform/tfplan

  terraform-apply:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: [terraform-validate, security-scan]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Terraform Init
        working-directory: ./terraform
        run: terraform init

      - name: Terraform Apply
        working-directory: ./terraform
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_region: ${{ env.GCP_REGION }}
          TF_VAR_environment: prod
          TF_VAR_enable_composer: false  # Set to true if needed
        run: terraform apply -auto-approve

      - name: Capture Outputs
        working-directory: ./terraform
        run: terraform output -json > outputs.json

      - name: Upload Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: terraform/outputs.json

  deploy-functions:
    name: Deploy Cloud Functions
    runs-on: ubuntu-latest
    needs: terraform-apply
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Package Data Ingestion Function
        working-directory: ./functions/data-ingestion
        run: |
          zip -r data-ingestion.zip main.py requirements.txt

      - name: Upload Data Ingestion Function
        run: |
          gsutil cp functions/data-ingestion/data-ingestion.zip \
            gs://${{ secrets.GCP_PROJECT_ID }}-function-source-prod/cloud-functions/

      - name: Package Pub/Sub Processor Function
        working-directory: ./functions/pubsub-processor
        run: |
          zip -r pubsub-processor.zip main.py requirements.txt

      - name: Upload Pub/Sub Processor Function
        run: |
          gsutil cp functions/pubsub-processor/pubsub-processor.zip \
            gs://${{ secrets.GCP_PROJECT_ID }}-function-source-prod/cloud-functions/

      - name: Update Cloud Functions
        run: |
          # Redeploy functions to use new code
          gcloud functions deploy data-ingestion-prod \
            --region=${{ env.GCP_REGION }} \
            --gen2 \
            --trigger-http

  deploy-pyspark-jobs:
    name: Deploy PySpark Jobs
    runs-on: ubuntu-latest
    needs: terraform-apply
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Upload PySpark Jobs
        run: |
          gsutil cp pyspark-jobs/*.py \
            gs://${{ secrets.GCP_PROJECT_ID }}-staging-prod/pyspark-jobs/
          gsutil cp pyspark-jobs/requirements.txt \
            gs://${{ secrets.GCP_PROJECT_ID }}-staging-prod/pyspark-jobs/

  deploy-airflow-dags:
    name: Deploy Airflow DAGs
    runs-on: ubuntu-latest
    needs: terraform-apply
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Get Composer DAG bucket
        id: get-dag-bucket
        run: |
          # Get the DAG bucket from Composer environment
          # Note: This assumes Composer is enabled
          DAG_BUCKET=$(gcloud composer environments describe data-pipeline-prod \
            --location ${{ env.GCP_REGION }} \
            --format="get(config.dagGcsPrefix)" || echo "")
          echo "dag_bucket=$DAG_BUCKET" >> $GITHUB_OUTPUT

      - name: Upload Airflow DAGs
        if: steps.get-dag-bucket.outputs.dag_bucket != ''
        run: |
          gsutil -m cp airflow-dags/*.py ${{ steps.get-dag-bucket.outputs.dag_bucket }}/

  test-pipeline:
    name: Test Pipeline
    runs-on: ubuntu-latest
    needs: [deploy-functions, deploy-pyspark-jobs]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Test Data Ingestion Function
        run: |
          FUNCTION_URL=$(gcloud functions describe data-ingestion-prod \
            --region=${{ env.GCP_REGION }} \
            --gen2 \
            --format="value(serviceConfig.uri)")
          
          curl -X POST "$FUNCTION_URL" \
            -H "Content-Type: application/json" \
            -d '{
              "data_type": "test",
              "payload": {
                "user_id": "test-user",
                "amount": 100.50,
                "category": "test-category"
              }
            }'

      - name: Verify BigQuery Tables
        run: |
          bq query --use_legacy_sql=false \
            'SELECT COUNT(*) as count FROM `${{ secrets.GCP_PROJECT_ID }}.data_warehouse_prod.raw_data` LIMIT 1'

  notify:
    name: Send Notification
    runs-on: ubuntu-latest
    needs: [test-pipeline]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Send Success Notification
        if: needs.test-pipeline.result == 'success'
        run: |
          echo "✅ Pipeline deployed successfully!"
          # Add Slack/email notification here if needed

      - name: Send Failure Notification
        if: needs.test-pipeline.result == 'failure'
        run: |
          echo "❌ Pipeline deployment failed!"
          # Add Slack/email notification here if needed
